{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eSJ2FO6NGzP"
      },
      "outputs": [],
      "source": [
        "import ast, random, spacy\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from spacy.tokens import DocBin\n",
        "from spacy.training import Example\n",
        "from spacy.util import minibatch, compounding\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Scorpio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = \"/content/NER_Dataset.csv\"\n",
        "df = pd.read_csv(CSV_PATH, encoding=\"latin-1\")\n",
        "\n",
        "df[\"tokens\"] = df[\"Word\"].apply(ast.literal_eval)\n",
        "df[\"tags\"]   = df[\"Tag\"].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "I7nHSl6qOEdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp  = spacy.blank(\"en\")\n",
        "ner  = nlp.add_pipe(\"ner\")\n",
        "\n",
        "labels = sorted({t.split(\"-\",1)[1] for row in df.tags for t in row if t != \"O\"})\n",
        "for lab in labels:\n",
        "    ner.add_label(lab)"
      ],
      "metadata": {
        "id": "eReV5iFsOMLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def row_to_example(tokens, tags):\n",
        "    text, offset, spans = \"\", 0, []\n",
        "    for tok, tag in zip(tokens, tags):\n",
        "        if text:\n",
        "            text += \" \"\n",
        "            offset += 1\n",
        "        start = offset\n",
        "        text += tok\n",
        "        end = offset + len(tok)\n",
        "        if tag != \"O\":\n",
        "            spans.append((start, end, tag.split(\"-\",1)[1]))\n",
        "        offset = end\n",
        "    doc = nlp.make_doc(text)\n",
        "    doc.ents = [doc.char_span(s, e, label=l, alignment_mode=\"contract\")\n",
        "                for s,e,l in spans if doc.char_span(s, e, label=l)]\n",
        "    return Example.from_dict(doc, {\"entities\": spans})\n",
        "\n",
        "examples = [row_to_example(toks, tags)\n",
        "            for toks, tags in zip(df.tokens, df.tags)]"
      ],
      "metadata": {
        "id": "FcJdx6JYOOwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_exs, tmp = train_test_split(examples, test_size=0.2,\n",
        "                                      random_state=42, shuffle=True)\n",
        "dev_exs, test_exs  = train_test_split(tmp, test_size=0.5,\n",
        "                                      random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "ZfROSCu8OR8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, exs in [(\"train\", train_exs), (\"dev\", dev_exs), (\"test\", test_exs)]:\n",
        "    db = DocBin(store_user_data=True)\n",
        "    for ex in exs:\n",
        "        db.add(ex.reference)\n",
        "    db.to_disk(f\"{name}.spacy\")"
      ],
      "metadata": {
        "id": "HIyBewgMOYcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = nlp.initialize(get_examples=lambda: train_exs)\n",
        "N_EPOCHS  = 37\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    random.shuffle(train_exs)\n",
        "    losses = {}\n",
        "\n",
        "    for batch in minibatch(train_exs, size=compounding(4.0, 32.0, 1.5)):\n",
        "        nlp.update(batch, sgd=optimizer, drop=0.2, losses=losses)\n",
        "\n",
        "    dev_examples = [Example(nlp(ex.reference.text), ex.reference)\n",
        "                    for ex in dev_exs]\n",
        "    dev_scores = nlp.evaluate(dev_examples)\n",
        "\n",
        "    print(\n",
        "        f\"epoch {epoch:02d}  \"\n",
        "        f\"loss={losses['ner']:.3f}  \"\n",
        "        f\"P={dev_scores['ents_p']:.2f}  \"\n",
        "        f\"R={dev_scores['ents_r']:.2f}  \"\n",
        "        f\"F1={dev_scores['ents_f']:.2f}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwH62PiSrEzG",
        "outputId": "d8aa450d-8910-48f8-9b75-4dc2dc2b8fa8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 01  loss=61284.378  P=0.85  R=0.84  F1=0.85\n",
            "epoch 02  loss=40865.917  P=0.86  R=0.86  F1=0.86\n",
            "epoch 03  loss=36770.088  P=0.87  R=0.85  F1=0.86\n",
            "epoch 04  loss=34194.790  P=0.87  R=0.86  F1=0.87\n",
            "epoch 05  loss=32499.618  P=0.87  R=0.86  F1=0.86\n",
            "epoch 06  loss=30931.744  P=0.87  R=0.85  F1=0.86\n",
            "epoch 07  loss=29772.791  P=0.88  R=0.86  F1=0.87\n",
            "epoch 08  loss=28540.543  P=0.88  R=0.86  F1=0.87\n",
            "epoch 09  loss=27717.044  P=0.87  R=0.87  F1=0.87\n",
            "epoch 10  loss=26981.188  P=0.87  R=0.88  F1=0.87\n",
            "epoch 11  loss=26049.764  P=0.88  R=0.87  F1=0.87\n",
            "epoch 12  loss=25342.150  P=0.88  R=0.87  F1=0.88\n",
            "epoch 13  loss=25053.035  P=0.88  R=0.87  F1=0.88\n",
            "epoch 14  loss=24272.777  P=0.88  R=0.87  F1=0.87\n",
            "epoch 15  loss=24019.191  P=0.88  R=0.87  F1=0.88\n",
            "epoch 16  loss=23758.517  P=0.88  R=0.86  F1=0.87\n",
            "epoch 17  loss=22932.794  P=0.88  R=0.87  F1=0.87\n",
            "epoch 18  loss=22706.040  P=0.87  R=0.87  F1=0.87\n",
            "epoch 19  loss=22285.696  P=0.88  R=0.87  F1=0.87\n",
            "epoch 20  loss=22076.073  P=0.88  R=0.87  F1=0.87\n",
            "epoch 21  loss=21827.875  P=0.88  R=0.88  F1=0.88\n",
            "epoch 22  loss=21475.015  P=0.88  R=0.87  F1=0.88\n",
            "epoch 23  loss=21251.983  P=0.87  R=0.87  F1=0.87\n",
            "epoch 24  loss=21088.358  P=0.88  R=0.87  F1=0.87\n",
            "epoch 25  loss=20600.513  P=0.88  R=0.86  F1=0.87\n",
            "epoch 26  loss=20382.578  P=0.87  R=0.87  F1=0.87\n",
            "epoch 27  loss=20137.234  P=0.87  R=0.87  F1=0.87\n",
            "epoch 28  loss=20013.516  P=0.87  R=0.88  F1=0.87\n",
            "epoch 29  loss=19700.743  P=0.87  R=0.87  F1=0.87\n",
            "epoch 30  loss=19893.982  P=0.88  R=0.87  F1=0.87\n",
            "epoch 31  loss=19321.334  P=0.88  R=0.87  F1=0.88\n",
            "epoch 32  loss=19228.873  P=0.88  R=0.87  F1=0.88\n",
            "epoch 33  loss=19071.235  P=0.87  R=0.88  F1=0.88\n",
            "epoch 34  loss=18901.478  P=0.87  R=0.88  F1=0.87\n",
            "epoch 35  loss=18655.764  P=0.87  R=0.88  F1=0.88\n",
            "epoch 36  loss=18410.063  P=0.88  R=0.87  F1=0.87\n",
            "epoch 37  loss=18355.927  P=0.86  R=0.88  F1=0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_score = nlp.evaluate([Example(nlp(ex.reference.text), ex.reference)\n",
        "                           for ex in test_exs])\n",
        "print(\"TEST  F1={ents_f:.2f}  P={ents_p:.2f}  R={ents_r:.2f}\".format(**test_score))"
      ],
      "metadata": {
        "id": "jzDgIatROonb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30106e1e-34b2-4354-8c81-b63ada9a90d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST  F1=0.87  P=0.87  R=0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = Path(\"best_ner_model\")\n",
        "nlp.to_disk(out_dir)\n",
        "print(f\"model saved to {out_dir.resolve()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq0Zcp3ZUvCv",
        "outputId": "62a497b9-2b90-4166-dca6-c2e06cce28f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model saved to /content/best_ner_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a small, human-editable template\n",
        "!python -m spacy init config base_config.cfg \\\n",
        "       --lang en \\\n",
        "       --pipeline ner \\\n",
        "       --optimize efficiency\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFe4cNwNU4mg",
        "outputId": "91897500-2dfd-4737-8ae2-022883eabc35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
            "install the spacy-transformers package and re-run this command. The config\n",
            "generated now does not use transformers.\u001b[0m\n",
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: en\n",
            "- Pipeline: ner\n",
            "- Optimize for: efficiency\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "base_config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train base_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOMHwj7bVvbp",
        "outputId": "9b51188c-940d-48a9-b901-d80113e8667e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ Nothing to auto-fill: base config is already complete\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg \\\n",
        "     --output ./output \\\n",
        "     --paths.train ./train.spacy \\\n",
        "     --paths.dev   ./dev.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVLxvLmpaUr-",
        "outputId": "41460f6e-d12d-4fea-de34-1f84e451e9e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     51.41    1.68    3.56    1.10    0.02\n",
            "  0     200         96.42   3189.72   68.91   71.05   66.89    0.69\n",
            "  0     400        281.63   2072.62   77.36   78.31   76.45    0.77\n",
            "  0     600        207.21   2145.63   79.06   79.30   78.81    0.79\n",
            "  0     800        247.47   2567.05   81.46   83.33   79.67    0.81\n",
            "  0    1000        292.26   2779.74   82.21   84.39   80.15    0.82\n",
            "  0    1200        332.93   3181.09   83.18   84.26   82.14    0.83\n",
            "  0    1400        396.53   3680.62   82.92   83.97   81.91    0.83\n",
            "  0    1600        474.97   4382.86   84.49   85.19   83.80    0.84\n",
            "  0    1800        575.15   5186.38   84.77   84.49   85.05    0.85\n",
            "  0    2000        685.25   6039.74   85.53   86.65   84.44    0.86\n",
            "  0    2200        815.91   7065.90   86.05   86.42   85.67    0.86\n",
            "  1    2400        919.83   7668.78   86.06   86.30   85.81    0.86\n",
            "  1    2600       1010.64   7453.91   86.27   86.96   85.58    0.86\n",
            "  1    2800       1138.07   7574.79   86.88   87.36   86.41    0.87\n",
            "  1    3000       1155.81   7594.25   86.56   86.96   86.17    0.87\n",
            "  2    3200       1124.60   7229.57   86.55   87.26   85.86    0.87\n",
            "  2    3400       1189.46   6612.28   86.62   86.98   86.26    0.87\n",
            "  2    3600       1229.13   6488.81   86.77   87.33   86.22    0.87\n",
            "  2    3800       1299.12   6754.46   86.33   86.27   86.39    0.86\n",
            "  2    4000       1223.05   6888.52   86.97   88.02   85.94    0.87\n",
            "  3    4200       1281.48   5986.29   86.75   86.73   86.77    0.87\n",
            "  3    4400       1322.74   5950.32   86.51   85.87   87.15    0.87\n",
            "  3    4600       1386.29   6230.80   86.93   86.70   87.17    0.87\n",
            "  3    4800       1350.61   6167.00   86.94   86.45   87.43    0.87\n",
            "  4    5000       1382.85   5612.05   86.85   86.69   87.00    0.87\n",
            "  4    5200       1409.93   5527.39   87.08   87.59   86.57    0.87\n",
            "  4    5400       1439.47   5591.08   87.15   87.71   86.61    0.87\n",
            "  4    5600       1446.69   5926.53   87.15   87.25   87.04    0.87\n",
            "  5    5800       1444.06   5619.43   87.31   87.29   87.33    0.87\n",
            "  5    6000       1413.93   4873.01   87.27   87.26   87.28    0.87\n",
            "  5    6200       1557.89   5394.36   87.12   87.03   87.22    0.87\n",
            "  5    6400       1652.20   5437.50   87.21   87.23   87.19    0.87\n",
            "  5    6600       1573.29   5514.11   87.18   87.37   87.00    0.87\n",
            "  6    6800       4690.25   4834.45   86.81   86.90   86.72    0.87\n",
            "  6    7000       1544.01   4715.78   86.03   85.61   86.44    0.86\n",
            "  6    7200       1664.00   5123.31   87.42   87.97   86.88    0.87\n",
            "  6    7400       1672.31   5209.77   87.03   86.49   87.57    0.87\n",
            "  7    7600       1607.28   4788.43   87.13   87.75   86.52    0.87\n",
            "  7    7800       1667.59   4563.01   87.14   87.84   86.44    0.87\n",
            "  7    8000       1831.79   4847.53   87.21   87.36   87.07    0.87\n",
            "  7    8200       1829.65   4923.19   87.16   87.22   87.10    0.87\n",
            "  8    8400       1758.15   4909.53   87.11   86.95   87.27    0.87\n",
            "  8    8600       1801.88   4250.43   87.04   87.35   86.74    0.87\n",
            "  8    8800       1864.98   4480.88   87.10   88.03   86.19    0.87\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy evaluate output/model-best ./test.spacy --output metrics.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuCSCsgracCn",
        "outputId": "5c07cfef-0b40-41ab-c43c-beb53f37576e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   88.25 \n",
            "NER R   86.87 \n",
            "NER F   87.55 \n",
            "SPEED   20269 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "          P       R       F\n",
            "gpe   96.10   93.59   94.83\n",
            "per   86.59   91.60   89.02\n",
            "geo   86.90   91.92   89.34\n",
            "tim   94.58   89.71   92.08\n",
            "org   84.78   74.98   79.58\n",
            "eve   48.94   39.66   43.81\n",
            "art    5.88    1.03    1.75\n",
            "nat   60.00   33.33   42.86\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to metrics.json\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-n14wAljb6k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}